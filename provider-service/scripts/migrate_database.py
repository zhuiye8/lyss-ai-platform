#!/usr/bin/env python3
"""
Provider Serviceæ•°æ®åº“è¿ç§»è„šæœ¬

ç”¨äºå°†Provider Serviceæ•°æ®åº“ç»“æ„è¿ç§»åˆ°æœ€æ–°çš„ç»Ÿä¸€è§„èŒƒï¼Œ
ç¡®ä¿ä¸å…¶ä»–æœåŠ¡çš„æ•°æ®åº“è®¾è®¡ä¿æŒä¸€è‡´ã€‚

ä½¿ç”¨æ–¹æ³•:
    python scripts/migrate_database.py [--dry-run] [--env development|production]

Author: Lyss AI Team
Created: 2025-01-22
Modified: 2025-01-22
"""

import asyncio
import sys
import os
from pathlib import Path
import argparse
import logging
from typing import List, Dict, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from sqlalchemy import text, inspect, Column, String, Integer, DateTime, JSON, Float, Boolean, Text, Index
from sqlalchemy.sql import func
from app.core.config import get_settings
from app.core.database import engine

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class DatabaseMigrator:
    """æ•°æ®åº“è¿ç§»å™¨"""
    
    def __init__(self, dry_run: bool = False):
        self.dry_run = dry_run
        self.migration_history = []
    
    def log_migration(self, operation: str, description: str):
        """è®°å½•è¿ç§»æ“ä½œ"""
        self.migration_history.append({
            'operation': operation,
            'description': description,
            'executed': not self.dry_run
        })
        
        if self.dry_run:
            logger.info(f"[DRY RUN] {operation}: {description}")
        else:
            logger.info(f"[EXECUTE] {operation}: {description}")
    
    def execute_sql(self, sql: str, description: str):
        """æ‰§è¡ŒSQLè¯­å¥"""
        self.log_migration("SQL", description)
        
        if not self.dry_run:
            try:
                with engine.connect() as conn:
                    conn.execute(text(sql))
                    conn.commit()
                logger.info("âœ… SQLæ‰§è¡ŒæˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ SQLæ‰§è¡Œå¤±è´¥: {e}")
                raise
    
    def check_table_exists(self, table_name: str) -> bool:
        """æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨"""
        try:
            inspector = inspect(engine)
            return table_name in inspector.get_table_names()
        except Exception as e:
            logger.error(f"æ£€æŸ¥è¡¨ {table_name} å¤±è´¥: {e}")
            return False
    
    def get_table_columns(self, table_name: str) -> List[Dict]:
        """è·å–è¡¨çš„åˆ—ä¿¡æ¯"""
        try:
            inspector = inspect(engine)
            return inspector.get_columns(table_name)
        except Exception as e:
            logger.error(f"è·å–è¡¨ {table_name} åˆ—ä¿¡æ¯å¤±è´¥: {e}")
            return []
    
    def migrate_channel_table(self):
        """è¿ç§»Channelè¡¨ä»¥ç¬¦åˆç»Ÿä¸€è§„èŒƒ"""
        logger.info("ğŸ”„ å¼€å§‹è¿ç§»Provider Channelè¡¨...")
        
        # æ£€æŸ¥ç°æœ‰è¡¨ç»“æ„
        if not self.check_table_exists('provider_channels'):
            logger.info("provider_channelsè¡¨ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°è¡¨...")
            
            create_sql = """
            CREATE TABLE provider_channels (
                id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                tenant_id UUID NOT NULL,
                name VARCHAR(100) NOT NULL,
                type INTEGER NOT NULL REFERENCES provider_types(id),
                
                -- One-APIæ ‡å‡†å­—æ®µ
                key TEXT NOT NULL,
                priority INTEGER DEFAULT 1,
                weight INTEGER DEFAULT 1,
                status INTEGER DEFAULT 1,
                
                -- é…é¢ç®¡ç†
                quota BIGINT DEFAULT -1,
                used_quota BIGINT DEFAULT 0,
                rate_limit INTEGER DEFAULT 100,
                
                -- é…ç½®èŒƒå›´
                config_scope VARCHAR(20) DEFAULT 'personal',
                owner_id UUID,
                group_id UUID,
                
                -- æ¨¡å‹é…ç½®
                models JSONB DEFAULT '[]',
                model_mapping JSONB DEFAULT '{}',
                
                created_at TIMESTAMP DEFAULT NOW(),
                updated_at TIMESTAMP DEFAULT NOW(),
                
                CONSTRAINT uk_provider_channels_tenant_name UNIQUE(tenant_id, name)
            );
            
            -- åˆ›å»ºç´¢å¼•
            CREATE INDEX idx_provider_channels_tenant_scope ON provider_channels(tenant_id, config_scope);
            CREATE INDEX idx_provider_channels_status ON provider_channels(status);
            CREATE INDEX idx_provider_channels_priority ON provider_channels(priority);
            """
            
            self.execute_sql(create_sql, "åˆ›å»ºç¬¦åˆç»Ÿä¸€è§„èŒƒçš„provider_channelsè¡¨")
            return
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ·»åŠ æ–°å­—æ®µ
        columns = self.get_table_columns('provider_channels')
        column_names = [col['name'] for col in columns]
        
        # éœ€è¦æ·»åŠ çš„å­—æ®µ
        required_fields = {
            'quota': 'BIGINT DEFAULT -1',
            'used_quota': 'BIGINT DEFAULT 0', 
            'rate_limit': 'INTEGER DEFAULT 100',
            'config_scope': 'VARCHAR(20) DEFAULT \'personal\'',
            'owner_id': 'UUID',
            'group_id': 'UUID',
            'model_mapping': 'JSONB DEFAULT \'{}\'',
            'type': 'INTEGER'
        }
        
        # æ·»åŠ ç¼ºå¤±å­—æ®µ
        for field_name, field_definition in required_fields.items():
            if field_name not in column_names:
                alter_sql = f"ALTER TABLE provider_channels ADD COLUMN {field_name} {field_definition}"
                self.execute_sql(alter_sql, f"æ·»åŠ å­—æ®µ {field_name}")
        
        # æ£€æŸ¥å¹¶åˆ›å»ºç¼ºå¤±çš„ç´¢å¼•
        self.create_missing_indexes('provider_channels', [
            "CREATE INDEX IF NOT EXISTS idx_provider_channels_tenant_scope ON provider_channels(tenant_id, config_scope)",
            "CREATE INDEX IF NOT EXISTS idx_provider_channels_priority ON provider_channels(priority)",
            "CREATE INDEX IF NOT EXISTS idx_provider_channels_quota ON provider_channels(quota)",
        ])
    
    def migrate_token_table(self):
        """è¿ç§»Tokenè¡¨ä»¥æ”¯æŒOne-APIå…¼å®¹"""
        logger.info("ğŸ”„ åˆ›å»ºç”¨æˆ·Tokenè¡¨...")
        
        if not self.check_table_exists('user_tokens'):
            create_sql = """
            CREATE TABLE user_tokens (
                id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
                tenant_id UUID NOT NULL,
                user_id UUID NOT NULL,
                name VARCHAR(100) NOT NULL,
                key TEXT UNIQUE NOT NULL,
                
                -- æƒé™æ§åˆ¶
                allowed_channels JSONB DEFAULT '[]',
                allowed_models JSONB DEFAULT '[]',
                
                -- é…é¢ç®¡ç†
                quota BIGINT DEFAULT -1,
                used_quota BIGINT DEFAULT 0,
                rate_limit INTEGER DEFAULT 60,
                
                -- çŠ¶æ€ç®¡ç†
                status INTEGER DEFAULT 1,
                expires_at TIMESTAMP,
                last_used_at TIMESTAMP,
                
                created_at TIMESTAMP DEFAULT NOW(),
                updated_at TIMESTAMP DEFAULT NOW(),
                
                CONSTRAINT uk_user_tokens_tenant_name UNIQUE(tenant_id, name)
            );
            
            -- åˆ›å»ºç´¢å¼•
            CREATE INDEX idx_user_tokens_tenant_user ON user_tokens(tenant_id, user_id);
            CREATE INDEX idx_user_tokens_key ON user_tokens(key);
            CREATE INDEX idx_user_tokens_status ON user_tokens(status);
            CREATE INDEX idx_user_tokens_expires ON user_tokens(expires_at);
            """
            
            self.execute_sql(create_sql, "åˆ›å»ºç”¨æˆ·Tokenè¡¨")
    
    def migrate_provider_types_table(self):
        """åˆ›å»ºä¾›åº”å•†ç±»å‹å®šä¹‰è¡¨"""
        logger.info("ğŸ”„ åˆ›å»ºä¾›åº”å•†ç±»å‹è¡¨...")
        
        if not self.check_table_exists('provider_types'):
            create_sql = """
            CREATE TABLE provider_types (
                id SERIAL PRIMARY KEY,
                name VARCHAR(50) UNIQUE NOT NULL,
                display_name VARCHAR(100) NOT NULL,
                base_url TEXT,
                supported_models JSONB DEFAULT '[]',
                api_format VARCHAR(20) DEFAULT 'openai',
                status VARCHAR(20) DEFAULT 'active',
                created_at TIMESTAMP DEFAULT NOW(),
                updated_at TIMESTAMP DEFAULT NOW()
            );
            
            -- æ’å…¥é¢„ç½®ä¾›åº”å•†ç±»å‹
            INSERT INTO provider_types (name, display_name, base_url, api_format) VALUES
            ('openai', 'OpenAI', 'https://api.openai.com', 'openai'),
            ('anthropic', 'Anthropic', 'https://api.anthropic.com', 'anthropic'),
            ('deepseek', 'DeepSeek', 'https://api.deepseek.com', 'openai'),
            ('azure-openai', 'Azure OpenAI', NULL, 'openai'),
            ('zhipu', 'æ™ºè°±AI', 'https://open.bigmodel.cn', 'openai'),
            ('google', 'Google AI', 'https://generativelanguage.googleapis.com', 'google');
            """
            
            self.execute_sql(create_sql, "åˆ›å»ºä¾›åº”å•†ç±»å‹è¡¨å¹¶æ’å…¥é¢„ç½®æ•°æ®")
    
    def migrate_usage_logs_table(self):
        """ä¼˜åŒ–è¯·æ±‚æ—¥å¿—è¡¨ç»“æ„"""
        logger.info("ğŸ”„ ä¼˜åŒ–è¯·æ±‚æ—¥å¿—è¡¨...")
        
        # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        if self.check_table_exists('provider_request_logs'):
            # æ·»åŠ åˆ†åŒºæ”¯æŒï¼ˆæŒ‰æœˆåˆ†åŒºï¼‰
            partition_sql = """
            -- ä¸ºå¤§æ•°æ®é‡è¡¨æ·»åŠ åˆ†åŒºæ”¯æŒ
            -- æŒ‰æœˆåˆ†åŒºprovider_request_logsè¡¨
            DO $$
            BEGIN
                IF NOT EXISTS (
                    SELECT 1 FROM pg_class WHERE relname = 'provider_request_logs_y2025m01'
                ) THEN
                    -- æ³¨æ„ï¼šè¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼Œå®é™…åˆ†åŒºéœ€è¦é‡å»ºè¡¨
                    -- åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éœ€è¦æ›´è°¨æ…çš„åˆ†åŒºç­–ç•¥
                    RAISE NOTICE 'éœ€è¦æ‰‹åŠ¨åˆ›å»ºåˆ†åŒºè¡¨ï¼Œè¯·å‚è€ƒæ–‡æ¡£';
                END IF;
            END $$;
            """
            
            self.execute_sql(partition_sql, "å‡†å¤‡è¯·æ±‚æ—¥å¿—è¡¨åˆ†åŒº")
    
    def create_missing_indexes(self, table_name: str, indexes: List[str]):
        """åˆ›å»ºç¼ºå¤±çš„ç´¢å¼•"""
        logger.info(f"æ£€æŸ¥è¡¨ {table_name} çš„ç´¢å¼•...")
        
        for index_sql in indexes:
            try:
                self.execute_sql(index_sql, f"ä¸º {table_name} åˆ›å»ºç´¢å¼•")
            except Exception as e:
                if "already exists" in str(e):
                    logger.info(f"ç´¢å¼•å·²å­˜åœ¨ï¼Œè·³è¿‡")
                else:
                    raise
    
    def optimize_existing_tables(self):
        """ä¼˜åŒ–ç°æœ‰è¡¨ç»“æ„"""
        logger.info("ğŸ”„ ä¼˜åŒ–ç°æœ‰è¡¨ç»“æ„...")
        
        # ä¸ºç°æœ‰è¡¨æ·»åŠ æ³¨é‡Š
        comments_sql = [
            "COMMENT ON TABLE provider_configs IS 'ä¾›åº”å•†é…ç½®è¡¨'",
            "COMMENT ON TABLE provider_channels IS 'Provider Channelé…ç½®è¡¨ï¼ˆå¤šç§Ÿæˆ·éš”ç¦»ï¼‰'",
            "COMMENT ON TABLE provider_request_logs IS 'è¯·æ±‚æ—¥å¿—è¡¨'",
            "COMMENT ON TABLE provider_channel_metrics IS 'Provider ChannelæŒ‡æ ‡ç»Ÿè®¡è¡¨'",
            "COMMENT ON COLUMN provider_channels.tenant_id IS 'ç§Ÿæˆ·UUID'",
            "COMMENT ON COLUMN provider_channels.credentials IS 'pgcryptoåŠ å¯†åçš„å‡­è¯ä¿¡æ¯'",
        ]
        
        for comment_sql in comments_sql:
            try:
                self.execute_sql(comment_sql, "æ·»åŠ è¡¨æ³¨é‡Š")
            except Exception as e:
                logger.warning(f"æ·»åŠ æ³¨é‡Šå¤±è´¥: {e}")
    
    def update_statistics(self):
        """æ›´æ–°æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        if not self.dry_run:
            logger.info("ğŸ”„ æ›´æ–°æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯...")
            
            analyze_sql = """
            ANALYZE provider_configs;
            ANALYZE provider_channels;
            ANALYZE provider_request_logs;
            ANALYZE provider_channel_metrics;
            """
            
            self.execute_sql(analyze_sql, "æ›´æ–°è¡¨ç»Ÿè®¡ä¿¡æ¯")
    
    def run_migration(self):
        """æ‰§è¡Œå®Œæ•´è¿ç§»"""
        logger.info("ğŸš€ å¼€å§‹Provider Serviceæ•°æ®åº“è¿ç§»...")
        
        try:
            # 1. åˆ›å»ºä¾›åº”å•†ç±»å‹è¡¨
            self.migrate_provider_types_table()
            
            # 2. è¿ç§»Channelè¡¨
            self.migrate_channel_table()
            
            # 3. åˆ›å»ºTokenè¡¨
            self.migrate_token_table()
            
            # 4. ä¼˜åŒ–è¯·æ±‚æ—¥å¿—è¡¨
            self.migrate_usage_logs_table()
            
            # 5. ä¼˜åŒ–ç°æœ‰è¡¨
            self.optimize_existing_tables()
            
            # 6. æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.update_statistics()
            
            # æ˜¾ç¤ºè¿ç§»æ‘˜è¦
            self.show_migration_summary()
            
            logger.info("ğŸ‰ Provider Serviceæ•°æ®åº“è¿ç§»å®Œæˆï¼")
            
        except Exception as e:
            logger.error(f"âŒ è¿ç§»è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            raise
    
    def show_migration_summary(self):
        """æ˜¾ç¤ºè¿ç§»æ‘˜è¦"""
        logger.info("\nğŸ“Š è¿ç§»æ“ä½œæ‘˜è¦:")
        logger.info("=" * 60)
        
        for item in self.migration_history:
            status = "âœ… å·²æ‰§è¡Œ" if item['executed'] else "ğŸ” é¢„è§ˆ"
            logger.info(f"{status} {item['operation']}: {item['description']}")
        
        logger.info("=" * 60)
        logger.info(f"æ€»æ“ä½œæ•°: {len(self.migration_history)}")


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='Provider Serviceæ•°æ®åº“è¿ç§»')
    parser.add_argument(
        '--dry-run', 
        action='store_true',
        help='é¢„è§ˆè¿ç§»æ“ä½œï¼Œä¸å®é™…æ‰§è¡Œ'
    )
    parser.add_argument(
        '--env', 
        choices=['development', 'production', 'testing'],
        default='development',
        help='ç¯å¢ƒç±»å‹ (default: development)'
    )
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    args = parse_arguments()
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    if args.env:
        os.environ['ENVIRONMENT'] = args.env
    
    if args.dry_run:
        logger.info("ğŸ” é¢„è§ˆæ¨¡å¼ - ä¸ä¼šå®é™…ä¿®æ”¹æ•°æ®åº“")
    
    logger.info(f"ğŸš€ Provider Serviceæ•°æ®åº“è¿ç§» - ç¯å¢ƒ: {args.env}")
    
    # æ£€æŸ¥æ•°æ®åº“è¿æ¥
    try:
        with engine.connect() as conn:
            conn.execute(text("SELECT 1")).scalar()
        logger.info("âœ… æ•°æ®åº“è¿æ¥æ­£å¸¸")
    except Exception as e:
        logger.error(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        sys.exit(1)
    
    # æ‰§è¡Œè¿ç§»
    migrator = DatabaseMigrator(dry_run=args.dry_run)
    
    try:
        migrator.run_migration()
        
        if args.dry_run:
            logger.info("ğŸ” é¢„è§ˆå®Œæˆï¼è¦å®é™…æ‰§è¡Œè¿ç§»ï¼Œè¯·ç§»é™¤ --dry-run å‚æ•°")
        else:
            logger.info("ğŸ‰ è¿ç§»æˆåŠŸå®Œæˆï¼")
            
    except Exception as e:
        logger.error(f"âŒ è¿ç§»å¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()