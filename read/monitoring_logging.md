# ç›‘æ§ä¸æ—¥å¿—ç³»ç»Ÿè®¾è®¡æ–‡æ¡£

## 1. ç›‘æ§æ¶æ„æ¦‚è¿°

### 1.1 ç›‘æ§ç­–ç•¥
- **å››ä¸ªé»„é‡‘ä¿¡å·**ï¼šå»¶è¿Ÿã€æµé‡ã€é”™è¯¯ç‡ã€é¥±å’Œåº¦
- **å¤šå±‚ç›‘æ§**ï¼šåŸºç¡€è®¾æ–½ã€åº”ç”¨ã€ä¸šåŠ¡ã€ç”¨æˆ·ä½“éªŒ
- **å®æ—¶å‘Šè­¦**ï¼šåŸºäºé˜ˆå€¼å’Œæœºå™¨å­¦ä¹ çš„æ™ºèƒ½å‘Šè­¦
- **å¯è§‚æµ‹æ€§**ï¼šMetricsã€Logsã€Tracesä¸‰ä½ä¸€ä½“

### 1.2 ç›‘æ§æŠ€æœ¯æ ˆ
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                æ•°æ®å±•ç¤ºå±‚                        â”‚
â”‚            Grafana Dashboard                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                å‘Šè­¦å¤„ç†å±‚                        â”‚
â”‚        AlertManager + PagerDuty               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                æ•°æ®å­˜å‚¨å±‚                        â”‚
â”‚    Prometheus + InfluxDB + Elasticsearch      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                æ•°æ®é‡‡é›†å±‚                        â”‚
â”‚   Node Exporter + cAdvisor + Jaeger           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                åº”ç”¨å±‚                           â”‚
â”‚        Lyss Platform Services                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 å…³é”®æŒ‡æ ‡ä½“ç³»
```python
# ç³»ç»Ÿå…³é”®æŒ‡æ ‡å®šä¹‰
SYSTEM_METRICS = {
    "availability": {
        "target": 99.95,  # 99.95%å¯ç”¨æ€§ç›®æ ‡
        "measurement": "uptime_ratio",
        "window": "monthly"
    },
    "performance": {
        "api_latency_p95": 200,  # 95%è¯·æ±‚å»¶è¿Ÿ<200ms
        "api_latency_p99": 500,  # 99%è¯·æ±‚å»¶è¿Ÿ<500ms
        "throughput": 5000       # æ”¯æŒ5000å¹¶å‘ç”¨æˆ·
    },
    "reliability": {
        "error_rate": 0.1,       # é”™è¯¯ç‡<0.1%
        "mttr": 15,              # å¹³å‡æ¢å¤æ—¶é—´<15åˆ†é’Ÿ
        "mtbf": 720              # å¹³å‡æ•…éšœé—´éš”>720å°æ—¶
    }
}
```

## 2. æŒ‡æ ‡æ”¶é›†ç³»ç»Ÿ

### 2.1 åº”ç”¨æŒ‡æ ‡
```python
# metrics/application.py
from prometheus_client import Counter, Histogram, Gauge, generate_latest
import time
from functools import wraps

# å®šä¹‰æ ¸å¿ƒæŒ‡æ ‡
REQUEST_COUNT = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status', 'tenant_id']
)

REQUEST_DURATION = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration',
    ['method', 'endpoint', 'tenant_id'],
    buckets=[0.1, 0.25, 0.5, 1.0, 2.0, 5.0, 10.0]
)

ACTIVE_CONNECTIONS = Gauge(
    'active_connections',
    'Active database connections',
    ['database', 'tenant_id']
)

AI_MODEL_REQUESTS = Counter(
    'ai_model_requests_total',
    'Total AI model requests',
    ['provider', 'model', 'tenant_id', 'status']
)

AI_MODEL_LATENCY = Histogram(
    'ai_model_latency_seconds',
    'AI model response latency',
    ['provider', 'model', 'tenant_id'],
    buckets=[0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0]
)

TOKEN_USAGE = Counter(
    'ai_tokens_consumed_total',
    'Total AI tokens consumed',
    ['provider', 'model', 'tenant_id', 'token_type']
)

# è£…é¥°å™¨ç”¨äºè‡ªåŠ¨æ”¶é›†æŒ‡æ ‡
def monitor_api(endpoint_name: str):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            tenant_id = kwargs.get('tenant_id', 'unknown')
            method = kwargs.get('method', 'unknown')
            
            start_time = time.time()
            try:
                result = await func(*args, **kwargs)
                REQUEST_COUNT.labels(
                    method=method,
                    endpoint=endpoint_name,
                    status='success',
                    tenant_id=tenant_id
                ).inc()
                return result
            except Exception as e:
                REQUEST_COUNT.labels(
                    method=method,
                    endpoint=endpoint_name,
                    status='error',
                    tenant_id=tenant_id
                ).inc()
                raise
            finally:
                duration = time.time() - start_time
                REQUEST_DURATION.labels(
                    method=method,
                    endpoint=endpoint_name,
                    tenant_id=tenant_id
                ).observe(duration)
        
        return wrapper
    return decorator

# ä¸šåŠ¡æŒ‡æ ‡æ”¶é›†å™¨
class BusinessMetrics:
    def __init__(self):
        self.user_sessions = Gauge(
            'active_user_sessions',
            'Active user sessions',
            ['tenant_id']
        )
        
        self.conversation_count = Counter(
            'conversations_created_total',
            'Total conversations created',
            ['tenant_id']
        )
        
        self.message_count = Counter(
            'messages_sent_total',
            'Total messages sent',
            ['tenant_id', 'message_type']
        )
        
        self.revenue_metrics = Counter(
            'revenue_generated_total',
            'Total revenue generated',
            ['tenant_id', 'plan_type']
        )
    
    def track_user_login(self, tenant_id: str):
        self.user_sessions.labels(tenant_id=tenant_id).inc()
    
    def track_user_logout(self, tenant_id: str):
        self.user_sessions.labels(tenant_id=tenant_id).dec()
    
    def track_conversation_created(self, tenant_id: str):
        self.conversation_count.labels(tenant_id=tenant_id).inc()
    
    def track_message_sent(self, tenant_id: str, message_type: str):
        self.message_count.labels(
            tenant_id=tenant_id,
            message_type=message_type
        ).inc()
    
    def track_ai_request(self, tenant_id: str, provider: str, 
                        model: str, tokens: int, success: bool):
        status = 'success' if success else 'error'
        AI_MODEL_REQUESTS.labels(
            provider=provider,
            model=model,
            tenant_id=tenant_id,
            status=status
        ).inc()
        
        if success:
            TOKEN_USAGE.labels(
                provider=provider,
                model=model,
                tenant_id=tenant_id,
                token_type='total'
            ).inc(tokens)

business_metrics = BusinessMetrics()
```

### 2.2 åŸºç¡€è®¾æ–½æŒ‡æ ‡
```yaml
# monitoring/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  # åº”ç”¨æŒ‡æ ‡
  - job_name: 'lyss-api-gateway'
    static_configs:
      - targets: ['api-gateway:8000']
    metrics_path: '/metrics'
    scrape_interval: 10s
    
  - job_name: 'lyss-eino-service'
    static_configs:
      - targets: ['eino-service:8080']
    metrics_path: '/metrics'
    scrape_interval: 10s
    
  # åŸºç¡€è®¾æ–½æŒ‡æ ‡
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']
    
  - job_name: 'postgres-exporter'
    static_configs:
      - targets: ['postgres-exporter:9187']
    
  - job_name: 'redis-exporter'
    static_configs:
      - targets: ['redis-exporter:9121']
    
  # å®¹å™¨æŒ‡æ ‡
  - job_name: 'cadvisor'
    static_configs:
      - targets: ['cadvisor:8080']
    
  # KubernetesæŒ‡æ ‡
  - job_name: 'kubernetes-apiservers'
    kubernetes_sd_configs:
      - role: endpoints
    relabel_configs:
      - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
        action: keep
        regex: default;kubernetes;https
```

## 3. æ—¥å¿—ç³»ç»Ÿ

### 3.1 æ—¥å¿—æ¶æ„
```
åº”ç”¨æ—¥å¿— â†’ Filebeat â†’ Logstash â†’ Elasticsearch â†’ Kibana
         â†“
      Fluentd â†’ InfluxDB â†’ Grafana
```

### 3.2 ç»“æ„åŒ–æ—¥å¿—
```python
# logging/structured_logger.py
import json
import logging
from datetime import datetime
from typing import Dict, Any, Optional
import traceback

class StructuredLogger:
    def __init__(self, service_name: str, version: str):
        self.service_name = service_name
        self.version = version
        self.logger = self._setup_logger()
    
    def _setup_logger(self):
        logger = logging.getLogger(self.service_name)
        logger.setLevel(logging.INFO)
        
        # JSONæ ¼å¼åŒ–å™¨
        formatter = StructuredFormatter(self.service_name, self.version)
        
        # æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)
        
        # æ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(f'/logs/{self.service_name}.log')
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)
        
        return logger
    
    def info(self, message: str, **kwargs):
        self._log('INFO', message, **kwargs)
    
    def warning(self, message: str, **kwargs):
        self._log('WARNING', message, **kwargs)
    
    def error(self, message: str, error: Exception = None, **kwargs):
        if error:
            kwargs['error'] = {
                'type': type(error).__name__,
                'message': str(error),
                'traceback': traceback.format_exc()
            }
        self._log('ERROR', message, **kwargs)
    
    def audit(self, event_type: str, tenant_id: str, user_id: str, 
              action: str, resource: str, result: str, **kwargs):
        self._log('AUDIT', f"Audit event: {event_type}", 
                  event_type=event_type,
                  tenant_id=tenant_id,
                  user_id=user_id,
                  action=action,
                  resource=resource,
                  result=result,
                  **kwargs)
    
    def security(self, event_type: str, severity: str, description: str, **kwargs):
        self._log('SECURITY', f"Security event: {event_type}",
                  event_type=event_type,
                  severity=severity,
                  description=description,
                  **kwargs)
    
    def _log(self, level: str, message: str, **kwargs):
        log_data = {
            'timestamp': datetime.utcnow().isoformat(),
            'service': self.service_name,
            'version': self.version,
            'level': level,
            'message': message,
            **kwargs
        }
        
        # æ ¹æ®çº§åˆ«é€‰æ‹©æ—¥å¿—æ–¹æ³•
        log_method = getattr(self.logger, level.lower(), self.logger.info)
        log_method(json.dumps(log_data, ensure_ascii=False))

class StructuredFormatter(logging.Formatter):
    def __init__(self, service_name: str, version: str):
        self.service_name = service_name
        self.version = version
        super().__init__()
    
    def format(self, record):
        # å¦‚æœrecord.msgå·²ç»æ˜¯JSONï¼Œç›´æ¥è¿”å›
        if hasattr(record, 'msg') and record.msg.startswith('{'):
            return record.msg
        
        # å¦åˆ™æ„å»ºç»“æ„åŒ–æ—¥å¿—
        log_data = {
            'timestamp': datetime.utcnow().isoformat(),
            'service': self.service_name,
            'version': self.version,
            'level': record.levelname,
            'message': record.getMessage(),
            'module': record.module,
            'function': record.funcName,
            'line': record.lineno
        }
        
        if record.exc_info:
            log_data['exception'] = self.formatException(record.exc_info)
        
        return json.dumps(log_data, ensure_ascii=False)

# å…¨å±€æ—¥å¿—å®ä¾‹
app_logger = StructuredLogger('lyss-platform', '1.0.0')
```

### 3.3 æ—¥å¿—ä¸­é—´ä»¶
```python
# middleware/logging_middleware.py
from fastapi import Request, Response
from starlette.middleware.base import BaseHTTPMiddleware
import time
import uuid

class LoggingMiddleware(BaseHTTPMiddleware):
    def __init__(self, app, logger):
        super().__init__(app)
        self.logger = logger
    
    async def dispatch(self, request: Request, call_next):
        # ç”Ÿæˆè¯·æ±‚ID
        request_id = str(uuid.uuid4())
        request.state.request_id = request_id
        
        # è¯·æ±‚å¼€å§‹æ—¥å¿—
        start_time = time.time()
        
        self.logger.info(
            "Request started",
            request_id=request_id,
            method=request.method,
            url=str(request.url),
            client_ip=request.client.host if request.client else None,
            user_agent=request.headers.get("user-agent"),
            tenant_id=getattr(request.state, 'tenant_id', None)
        )
        
        try:
            response = await call_next(request)
            
            # è¯·æ±‚å®Œæˆæ—¥å¿—
            duration = time.time() - start_time
            self.logger.info(
                "Request completed",
                request_id=request_id,
                status_code=response.status_code,
                duration_ms=round(duration * 1000, 2),
                response_size=response.headers.get("content-length")
            )
            
            # æ·»åŠ è¯·æ±‚IDåˆ°å“åº”å¤´
            response.headers["X-Request-ID"] = request_id
            
            return response
            
        except Exception as e:
            # è¯·æ±‚å¤±è´¥æ—¥å¿—
            duration = time.time() - start_time
            self.logger.error(
                "Request failed",
                error=e,
                request_id=request_id,
                duration_ms=round(duration * 1000, 2)
            )
            raise
```

## 4. åˆ†å¸ƒå¼è¿½è¸ª

### 4.1 Jaegeré…ç½®
```python
# tracing/jaeger_config.py
from opentelemetry import trace
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
from opentelemetry.instrumentation.requests import RequestsInstrumentor
from opentelemetry.instrumentation.sqlalchemy import SQLAlchemyInstrumentor

def setup_tracing(service_name: str, jaeger_endpoint: str):
    # è®¾ç½®tracer provider
    trace.set_tracer_provider(TracerProvider())
    tracer = trace.get_tracer(__name__)
    
    # Jaeger exporter
    jaeger_exporter = JaegerExporter(
        agent_host_name="jaeger-agent",
        agent_port=6831,
        collector_endpoint=jaeger_endpoint,
    )
    
    # Span processor
    span_processor = BatchSpanProcessor(jaeger_exporter)
    trace.get_tracer_provider().add_span_processor(span_processor)
    
    # è‡ªåŠ¨instrumentation
    FastAPIInstrumentor.instrument()
    RequestsInstrumentor.instrument()
    SQLAlchemyInstrumentor.instrument()
    
    return tracer

# è‡ªå®šä¹‰è¿½è¸ªè£…é¥°å™¨
def trace_function(operation_name: str):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            tracer = trace.get_tracer(__name__)
            with tracer.start_as_current_span(operation_name) as span:
                # æ·»åŠ spanå±æ€§
                span.set_attribute("function.name", func.__name__)
                span.set_attribute("function.module", func.__module__)
                
                try:
                    result = await func(*args, **kwargs)
                    span.set_attribute("function.result", "success")
                    return result
                except Exception as e:
                    span.set_attribute("function.result", "error")
                    span.set_attribute("error.type", type(e).__name__)
                    span.set_attribute("error.message", str(e))
                    raise
        return wrapper
    return decorator
```

## 5. å‘Šè­¦ç³»ç»Ÿ

### 5.1 å‘Šè­¦è§„åˆ™
```yaml
# monitoring/alert_rules.yml
groups:
- name: lyss_platform_alerts
  rules:
  # æœåŠ¡å¯ç”¨æ€§å‘Šè­¦
  - alert: ServiceDown
    expr: up == 0
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "Service {{ $labels.job }} is down"
      description: "Service {{ $labels.job }} has been down for more than 1 minute"
  
  # APIå»¶è¿Ÿå‘Šè­¦
  - alert: HighAPILatency
    expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 0.2
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High API latency detected"
      description: "95th percentile latency is {{ $value }}s for {{ $labels.endpoint }}"
  
  # é”™è¯¯ç‡å‘Šè­¦
  - alert: HighErrorRate
    expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) > 0.01
    for: 2m
    labels:
      severity: critical
    annotations:
      summary: "High error rate detected"
      description: "Error rate is {{ $value | humanizePercentage }} for {{ $labels.endpoint }}"
  
  # æ•°æ®åº“è¿æ¥å‘Šè­¦
  - alert: DatabaseConnectionHigh
    expr: active_connections > 80
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High database connection count"
      description: "Database {{ $labels.database }} has {{ $value }} active connections"
  
  # å†…å­˜ä½¿ç”¨ç‡å‘Šè­¦
  - alert: HighMemoryUsage
    expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes > 0.9
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "High memory usage"
      description: "Memory usage is {{ $value | humanizePercentage }} on {{ $labels.instance }}"
  
  # AIæ¨¡å‹è°ƒç”¨å¤±è´¥å‘Šè­¦
  - alert: AIModelFailures
    expr: rate(ai_model_requests_total{status="error"}[5m]) > 0.1
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "AI model failures detected"
      description: "{{ $labels.provider }}/{{ $labels.model }} failure rate is {{ $value }}/s"
  
  # ç§Ÿæˆ·é…é¢å‘Šè­¦
  - alert: TenantQuotaExceeded
    expr: rate(http_requests_total{status="429"}[5m]) > 0
    for: 1m
    labels:
      severity: warning
    annotations:
      summary: "Tenant quota exceeded"
      description: "Tenant {{ $labels.tenant_id }} is hitting rate limits"
```

### 5.2 å‘Šè­¦ç®¡ç†å™¨é…ç½®
```yaml
# monitoring/alertmanager.yml
global:
  smtp_smarthost: 'smtp.gmail.com:587'
  smtp_from: 'alerts@lyss.ai'

route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'default-receiver'
  routes:
  - match:
      severity: critical
    receiver: 'critical-alerts'
  - match:
      severity: warning
    receiver: 'warning-alerts'

receivers:
- name: 'default-receiver'
  slack_configs:
  - api_url: 'YOUR_SLACK_WEBHOOK_URL'
    channel: '#alerts'
    title: 'Lyss Platform Alert'
    text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'

- name: 'critical-alerts'
  slack_configs:
  - api_url: 'YOUR_SLACK_WEBHOOK_URL'
    channel: '#critical-alerts'
    title: 'ğŸš¨ CRITICAL: Lyss Platform Alert'
    text: '{{ range .Alerts }}{{ .Annotations.summary }}\n{{ .Annotations.description }}{{ end }}'
  pagerduty_configs:
  - service_key: 'YOUR_PAGERDUTY_SERVICE_KEY'
    description: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'

- name: 'warning-alerts'
  slack_configs:
  - api_url: 'YOUR_SLACK_WEBHOOK_URL'
    channel: '#warnings'
    title: 'âš ï¸ WARNING: Lyss Platform Alert'
    text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'

inhibit_rules:
- source_match:
    severity: 'critical'
  target_match:
    severity: 'warning'
  equal: ['alertname', 'cluster', 'service']
```

## 6. ä»ªè¡¨ç›˜é…ç½®

### 6.1 Grafanaä»ªè¡¨ç›˜
```json
{
  "dashboard": {
    "title": "Lyss Platform Overview",
    "panels": [
      {
        "title": "API Request Rate",
        "type": "stat",
        "targets": [
          {
            "expr": "sum(rate(http_requests_total[5m]))",
            "legendFormat": "Requests/sec"
          }
        ]
      },
      {
        "title": "API Latency Percentiles",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.50, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "50th percentile"
          },
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile"
          },
          {
            "expr": "histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "99th percentile"
          }
        ]
      },
      {
        "title": "Error Rate by Endpoint",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(rate(http_requests_total{status=~\"5..\"}[5m])) by (endpoint) / sum(rate(http_requests_total[5m])) by (endpoint)",
            "legendFormat": "{{ endpoint }}"
          }
        ]
      },
      {
        "title": "Active Users by Tenant",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(active_user_sessions) by (tenant_id)",
            "legendFormat": "{{ tenant_id }}"
          }
        ]
      },
      {
        "title": "AI Model Usage",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(rate(ai_model_requests_total[5m])) by (provider, model)",
            "legendFormat": "{{ provider }}/{{ model }}"
          }
        ]
      },
      {
        "title": "Token Consumption",
        "type": "graph",
        "targets": [
          {
            "expr": "sum(rate(ai_tokens_consumed_total[5m])) by (provider)",
            "legendFormat": "{{ provider }}"
          }
        ]
      }
    ]
  }
}
```

## 7. æ—¥å¿—åˆ†ææŸ¥è¯¢

### 7.1 å¸¸ç”¨ELKæŸ¥è¯¢
```json
{
  "kibana_queries": {
    "error_analysis": {
      "query": {
        "bool": {
          "must": [
            {"match": {"level": "ERROR"}},
            {"range": {"timestamp": {"gte": "now-1h"}}}
          ]
        }
      },
      "aggs": {
        "error_by_service": {
          "terms": {"field": "service.keyword"}
        }
      }
    },
    "security_events": {
      "query": {
        "bool": {
          "must": [
            {"match": {"level": "SECURITY"}},
            {"match": {"severity": "high"}}
          ]
        }
      }
    },
    "slow_queries": {
      "query": {
        "bool": {
          "must": [
            {"match": {"message": "slow query"}},
            {"range": {"duration_ms": {"gte": 1000}}}
          ]
        }
      }
    },
    "tenant_activity": {
      "query": {
        "bool": {
          "must": [
            {"term": {"tenant_id": "TENANT_ID_HERE"}},
            {"range": {"timestamp": {"gte": "now-24h"}}}
          ]
        }
      },
      "aggs": {
        "activity_by_hour": {
          "date_histogram": {
            "field": "timestamp",
            "interval": "1h"
          }
        }
      }
    }
  }
}
```

## 8. ç›‘æ§è‡ªåŠ¨åŒ–

### 8.1 å¥åº·æ£€æŸ¥è„šæœ¬
```python
# scripts/health_check.py
import asyncio
import aiohttp
import sys
from datetime import datetime

class HealthChecker:
    def __init__(self):
        self.services = {
            'api-gateway': 'http://api-gateway:8000/health',
            'eino-service': 'http://eino-service:8080/health',
            'memory-service': 'http://memory-service:8001/health'
        }
        self.dependencies = {
            'postgres': 'postgresql://user:pass@postgres:5432/db',
            'redis': 'redis://redis:6379'
        }
    
    async def check_service_health(self, name: str, url: str):
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, timeout=5) as response:
                    if response.status == 200:
                        data = await response.json()
                        return {
                            'service': name,
                            'status': 'healthy',
                            'response_time': data.get('response_time', 0),
                            'timestamp': datetime.utcnow().isoformat()
                        }
                    else:
                        return {
                            'service': name,
                            'status': 'unhealthy',
                            'error': f'HTTP {response.status}',
                            'timestamp': datetime.utcnow().isoformat()
                        }
        except Exception as e:
            return {
                'service': name,
                'status': 'unhealthy',
                'error': str(e),
                'timestamp': datetime.utcnow().isoformat()
            }
    
    async def run_health_checks(self):
        tasks = []
        for name, url in self.services.items():
            tasks.append(self.check_service_health(name, url))
        
        results = await asyncio.gather(*tasks)
        
        # æ£€æŸ¥æ•´ä½“å¥åº·çŠ¶æ€
        all_healthy = all(result['status'] == 'healthy' for result in results)
        
        health_report = {
            'overall_status': 'healthy' if all_healthy else 'unhealthy',
            'timestamp': datetime.utcnow().isoformat(),
            'services': results
        }
        
        print(json.dumps(health_report, indent=2))
        
        # å¦‚æœæœ‰æœåŠ¡ä¸å¥åº·ï¼Œè¿”å›éé›¶é€€å‡ºç 
        if not all_healthy:
            sys.exit(1)

if __name__ == '__main__':
    checker = HealthChecker()
    asyncio.run(checker.run_health_checks())
```

è¿™ä¸ªç›‘æ§ä¸æ—¥å¿—ç³»ç»Ÿè®¾è®¡æ–‡æ¡£æä¾›äº†å®Œæ•´çš„å¯è§‚æµ‹æ€§è§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬æŒ‡æ ‡æ”¶é›†ã€æ—¥å¿—ç®¡ç†ã€åˆ†å¸ƒå¼è¿½è¸ªã€å‘Šè­¦æœºåˆ¶å’Œå¯è§†åŒ–å±•ç¤ºç­‰æ ¸å¿ƒç»„ä»¶ã€‚